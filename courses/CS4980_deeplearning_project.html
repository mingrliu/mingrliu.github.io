<!--A Design by W3layouts
Author: W3layout
Author URL: http://w3layouts.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->
<!DOCTYPE HTML>
<html>
<head>
<title>CS4980: Deep Learning Project</title>
<link href="../css/bootstrap.css" rel='stylesheet' type='text/css' />
<!-- jQuery (necessary JavaScript plugins) -->
<script src="../js/jquery.min.js"></script>
<!-- Custom Theme files -->
 <link href="../css/dashboard.css" rel="stylesheet">
<link href="../css/style.css" rel='stylesheet' type='text/css' />

<!-- Custom Theme files -->
<!--//theme-style-->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Mingrui Liu Publications Papers Journal Conference" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
<link href='http://fonts.googleapis.com/css?family=Ubuntu:300,400,500,700' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Varela+Round' rel='stylesheet' type='text/css'>
<!-- start menu -->

<body>
<style type="../text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-yw4l{vertical-align:top}
</style>
<br>
</br>
<h3 class="clr2">Project Title: On the Training of Generative Adversarial Nets</h3>

<p>Team Members: Mingrui Liu, Xin Man</p>

<br>
</br>
<h3 class="clr3">Project Idea</h3>
Standard Generative Adversarial Net (GAN) suffers from the instablity and failure to achieve convergence during the training phase. To address this issue, there are two common strategies. The first is to change the objective function in a smart way to make it easier to train while maintaining its the effectiveness, and another way is to utilize different optimization algorithms to accelerate the training procedure. In this project, we aim to investigate and design different approaches for training GANs based on the two strategies. Concretely, we target on following recent developments of different GANs, and investigate a variety of optimization algorithms to facilitate efficient training. 
<br>
</br>
<h3 class="clr1">Dataset Details</h3>
There are many ideally existing datasets available, including MNIST, the Toronto Face Database (TFD), CIFAR-10, etc.
<br>
</br>
<h3 class="clr2">Software</h3>
All codes will be developed under the pytorch framework. We will implement different network architectures and optimization algorithms.
<br>
</br>
<h3 class="clr3">Relevant Papers</h3>
<ul>
<li>
    1. Generative Adversarial Nets, Ian J. Goodfellow et al, NIPS 2014.
    </li>
 <li>
2. Towards Principled Methods for Training Generative Adversarial Networks, Martin Arjovsky and Leon Bottou, ICLR 2017.
</li>
<li>
3. Wasserstein Generative Adversarial Networks, Martin Arjovsky et al, ICML 2017.
    </li>
<li>
4. Improved Training of Wasserstein GANs, Ishaan Gulrajani et al, NIPS 2017.
    </li>
</ul>
<br>
</br>
<h3 class="clr1">Progress Milestones</h3>
    <ul>
    <li>
        1. Before April 1st, get experimental results for GAN and WGAN, and (probably) another variant of GAN by using standard primal-dual mini-batch SGD update.        
        </li>
        <li>
        2. Before May 1st, get experimental results by using different optimization algorithms, including ADAM, SGD, etc. In addition, we try to get some theoretical understanding of different algorithms.
        </li>
        </ul>
        <br>
        </br>
</body>
</html>
