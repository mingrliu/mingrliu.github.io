<!--A Design by W3layouts
Author: W3layout
Author URL: http://w3layouts.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->
<html>
<head>
<title>Welcome to Mingrui Liu's Homepage</title>
<link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
<!-- jQuery (necessary JavaScript plugins) -->
<script src="js/jquery.min.js"></script>
<!-- Custom Theme files -->
 <link href="css/dashboard.css" rel="stylesheet">
<link href="css/style.css" rel='stylesheet' type='text/css' />

<!-- Custom Theme files -->
<!--//theme-style-->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Curriculum Vitae Responsive web template, Bootstrap Web Templates, Flat Web Templates, Andriod Compatible web template, 
Smartphone Compatible web template, free webdesigns for Nokia, Samsung, LG, SonyErricsson, Motorola web design" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
<link href='http://fonts.googleapis.com/css?family=Ubuntu:300,400,500,700' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Varela+Round' rel='stylesheet' type='text/css'>

<!-- start menu -->
  
</head>
<body>
<!-- header -->
<div class="col-sm-3 col-md-2 sidebar">
		 <div class="sidebar_top">
			 <h1>Mingrui Liu</h1> 
			 <a href="photo.html"><img src="images/photo1.jpg" alt=""/></a>
		 </div>
		<div class="details">
			 <h3>Ph.D.</h3>
			 <p>
             <span> Department of Computer Science</span><br/>
             <span> George Mason University </span><br/>
             <span> Fairfax, VA, 22030</span><br/>
             <span> USA <br/></p>
             <!--<span>Department of Computer Science</span><br/>
			 <span>The University of Iowa</span><br/>
			 <span>Iowa City, 52242</span><br/>
             <span>USA</span><br/></p>-->   
			 <h3>EMAIL</h3>
            <!--< <p>mingrliu (at) bu (dot) edu</p>-->
             <p>mingruiliu (dot) ml (at) gmail.com </p>
             <p>mingruil (at) gmu (dot) edu </p>

		</div>
		<div class="clearfix"></div>
</div>
<!---->
<link href="css/popuo-box.css" rel="stylesheet" type="text/css" media="all"/>
<script src="js/jquery.magnific-popup.js" type="text/javascript"></script>
	<!---//pop-up-box---->			
<div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
	 <div class="content">
		 <div class="details_header">
			 <ul>
	           <li><a href="index.html"><font size=4.5><b>Home</b></font></a></li>
                    <li><a href="people.html"><font size=4.5><b>People</b></font></a></li>
                     <li><a href="publications.html"><font size=4.5><b>Publications</b></font></a></li>
                     <li><a href="teaching.html"><font size=4.5><b>Teaching</b></font></a></li>
            <li><a href="software.html"><font size=4.5><b>Software</b></font></a></li>
                         <li><a href="service.html"><font size=4.5><b>Service</b></font></a></li>
                        <li><a href="awards.html"><font size=4.5><b>Awards</b></font></a></li>
                     <li><a href="miscellaneous.html"><font size=4.5><b>Miscellaneous</b></font></a></li>


			<!--	 <li><a href="cv/MingruiLiu_CV.pdf"><b>CV</b></a></li>
            <li><a href="#"><b>Miscellaneous</b></a></li>-->
			 </ul>
		 </div>
		 
		 <div class="company">
			 <h3 class="clr1">About Me</h3>
			 <div class="company_details">
                 <!--I am an assistant professor at <a href="https://www.cs.gmu.edu"><u><font color="#0000FF"> Department of Computer Science</u></a></font>, <a href="http://www.gmu.edu"><u><font color="#0000FF"> George Mason University</u></a></font>. Before that I was a postdoc at <a href="https://www.bu.edu/hic/"><u><font color="#0000FF">Rafik B. Hariri Institute</u></a></font> at <a href=https://www.bu.edu/><u><font color="#0000FF">Boston University</u></a></font>, hosted by <a href="http://francesco.orabona.com"><u><font color="#0000FF"> Francesco Orabona</u></a></font>. I received my Ph.D. at <a href="https://www.cs.uiowa.edu"><u><font color="#0000FF"> Department of Computer Science</u></a></font>, <a href="http://www.uiowa.edu"><u><font color="#0000FF"> The University of Iowa</u></a></font> in August 2020, under the advise of <a href="http://homepage.cs.uiowa.edu/~tyng/"><font color="#0000FF"><u>Tianbao Yang</u></a></font>. I have also spent time working at industrial research labs, such as IBM research AI and Alibaba DAMO Academy. Here is my <a href="https://scholar.google.com/citations?user=KFoEnFQAAAAJ&hl=en"><font color="#0000FF"><u>Google Scholar Citations</u></a></font>.  <br><br>-->

               I am an assistant professor at <a href="https://www.cs.gmu.edu"><u><font color="#0000FF"> Department of Computer Science</u></a></font>, <a href="http://www.gmu.edu"><u><font color="#0000FF"> George Mason University</u></a></font> since Fall 2021. Before that I was a postdoc at <a href="https://www.bu.edu/hic/"><u><font color="#0000FF">Rafik B. Hariri Institute</u></a></font> at Boston University from 2020-2021, hosted by <a href="http://francesco.orabona.com"><u><font color="#0000FF"> Francesco Orabona</u></a></font>. I received my Ph.D. at Department of Computer Science, The University of Iowa in August 2020, under the advise of <a href="http://homepage.cs.uiowa.edu/~tyng/"><font color="#0000FF"><u>Tianbao Yang</u></a></font>. Before that I studied at Institute of Natural Sciences and School of Mathematical Sciences at Shanghai Jiao Tong University. I have also spent time working at industrial research labs, such as IBM research AI and Alibaba DAMO Academy.  Here is my <a href="https://scholar.google.com/citations?user=KFoEnFQAAAAJ&hl=en"><font color="#0000FF"><u>Google Scholar Citations</u></a></font>.  <br><br>              


                 
                <FONT COLOR="#FC0404"> I am looking for self-motivated PhD students (fully-funded) with strong mathematical ablities and (or) programming skills to solve challenging machine learning problems elegantly with mathematical analysis and empirical studies. The main technical skills we need include mathematical optimization, statistical learning theory, algorithms, and deep learning. If you are interested, please drop me an email with your CV and transcript, and apply our PhD program <a href="https://cs.gmu.edu/prospective-students/phd-program/"><u><font color="#0000FF">here</u></a></font>. Undergrad and graduate student visitors are also welcome. <a href="https://github.com/dynaroars/dynaroars.github.io/wiki/About-GMU"><u><font color="#0000FF">This link</u></a></font> provides an overview of our fast-growing GMU CS department. </font>





                 

            </div>
        </div>  


		 <div class="skills">
			 <h3 class="clr1">Research</h3>
			 <div class="skill_info">
                 My research interests are machine learning, optimization, learning theory, and deep learning. My goal is to design provably efficient algorithms for machine learning problems with strong empirical performance. In particular, I work on   <br><br>

                <li>  <b>Mathematical Optimization for Machine Learning</b>: I design and analyze optimization algorithms with rigorous efficiency guarantees for modern learning problems, including the training of language models such as Transformers <a href="https://arxiv.org/pdf/2208.11195.pdf"><font color="#0000FF">[NeurIPS'22]</font></a>, hierarchical optimization (e.g., learning problems with minimax and bilevel formulation) [<a href="https://arxiv.org/pdf/2401.09587.pdf"><font color="#0000FF">ICLR'24 Spotlight</font></a>, <a href="https://openreview.net/pdf?id=36rWa8zVkh"><font color="#0000FF">ICML'24</font></a>, <a href="https://jmlr.csail.mit.edu/papers/volume22/20-533/20-533.pdf"><font color="#0000FF">JMLR'21</font></a>], optimization methods with adaptivity guarantees and its fundamental limits [<a href="https://openreview.net/pdf?id=ZjOXuAfS6l"><font color="#0000FF">ICLR'25</font></a>, <a href="https://openreview.net/forum?id=SJxIm0VtwH"><font color="#0000FF">ICLR'20</font></a>], and global convergence of neural network optimization [<a href="https://openreview.net/pdf?id=gVLKXT9JwG"><font color="#0000FF">NeurIPS'23</font></a>]. <br><br></li> 

                <li>  <b>Statistical Learning Theory</b>: I work on improving sample complexity and computational complexity for modern machine learning problems, such as the generalization guarantees of pairwise learning [<a href="https://openreview.net/pdf?id=lHFCaetMa_"><font color="#0000FF">NeurIPS'21</font></a>], and feature learning theory for neural networks[<a href="https://openreview.net/pdf?id=yHRxnhKyEJ"><font color="#0000FF">ICML'24</font></a>].  <br><br></li> 

                 <li> <b> Large-scale Distributed Optimization and Learning</b>: I design scalable algorithms for distributed intelligence that remain efficient under real-world constraints, such as limited communication [<a href="https://arxiv.org/pdf/2205.05040.pdf"><font color="#0000FF">NeurIPS'22 Spotlight</font>], decentralization [<a href="https://arxiv.org/abs/1910.12999.pdf"><font color="#0000FF">NeurIPS'20</font></a>], and node unavailability [<a href="https://arxiv.org/pdf/2410.23131"><font color="#0000FF">NeurIPS'24</font></a>]. Recently, I focus on moving beyond black-box analyses to develop principled, fine-grained theoretical frameworks for distributed learning problems [<a href="https://arxiv.org/pdf/2506.13974"><font color="#0000FF">ICML'25</font></a>, <a href="https://arxiv.org/pdf/2501.13790"><font color="#0000FF">ICLR'25</font></a>]<br><br></li> 

                 <li> <b>Machine Learning Applications</b>: sample selection [<a href="https://openreview.net/pdf?id=2dtU9ZbgSN"><font color="#0000FF">NeurIPS'23</font></a>], continual learning [<a href="https://arxiv.org/abs/1909.11763"><font color="#0000FF">NeurIPS'20 Spotlight</font></a>], parameter-efficient tuning of foundation models. <br><br></li> 
		    </div>
         </div>
<div class="company">
                         <h3 class="clr1"> Recent News</h3>
                         <div class="skill_info">
                         <ul>
                <li> (May 2025) One paper was accepted by ICML 2025. Congratulations to my student Michael and our collaborators!
                <li> (March/April 2025) Glad to give invited talks about "Beyond Black-Box Analysis: Understanding the Role of Local Updates in Distributed Learning" at UMN and KAUST. Check the <a href="https://www.youtube.com/watch?v=lncMHLmOopg"><font color="#0000FF">video</font></a>.
                <li> (Feb 2025) I will be serving as an Area Chair for NeurIPS 2025.
                <li> (Jan 2025) Two papers were accepted by ICLR 2025. Congratulations to my student Michael and our collaborators!
                <li> (Nov 2024) I will be serving as an Area Chair for ICML 2025. </li>
        <li> (Sep 2024) Two papers were accepted by NeurIPS 2024. Congratulations to my students Michael, Xiaochuan, and Jie! </li>
        <li> (Sep 2024) I will be serving as an Area Chair for AISTATS 2025. </li>
    <li> (Aug 2024) Glad to give an invited talk at Lehigh University. </li>
    <li> (May 2024) I will be serving as an Area Chair for NeurIPS 2024. </li>
    <li> (May 2024) Two papers were accepted by ICML 2024. Congratulations to my students!</li>
	<li> (Feb 2024) Glad to give an invited talk at Virginia Tech CS Seminar Series about our recent work on optimization for deep autoregressive models.</li>
	<li>  (Jan 2024) One paper about <a href="https://arxiv.org/pdf/2401.09587.pdf"><font color="#0000FF">bilevel optimization under unbounded smoothness</font></a> was accepted by ICLR 2024 as an spotlight (5% acceptance rate). Congrats to my students Jie and Xiaochuan!</li>
            <li><a href="news.html"><font color="#0000FF"> More News</a></font> </li>
  <!--<          <li> (Dec 2023) I was selected in the <a href="https://aaai.org/aaai-conference/nfh-24-program/" target="_blank"><font color="#0000FF">New Faculty Highlights Program</font></a> at AAAI 2024. I will present our recent work on algorithmic foundation of federated learning with sequential data.</li>
				<li> (Oct 2023) Glad to give invited talks at INFORMS annual meeting, Department of Mathematical Sciences at Rensseleaer Polytechic Institute about our recent work on optimization for unbounded smooth functions. Here is the <a href="talks/talk_gradient_clipping.pdf" target="_blank"><font color="#0000FF">slides</font></a>.</li>
		<li> (Sep 2023) Three papers were accepted by NeurIPS 2023. Congratulations to Michael, Jie and Yajie!</li>
				<li> (Sep 2023) Glad to give an invited talk at Business Analytics Department, University of Iowa.</li>
		<li> (Aug 2023) I will be serving as an Area Chair for AISTATS 2024.</li>
               <li> (Apr 2023) I am happy to give an invited talk at Thomas Jefferson High School of Science and Technology about "Federated Learning: Algorithm Design and Applications". Here is the <a href="talks/talk_federated_learning_TJ_April_22.pdf" target="_blank"><font color="#0000FF">slides</font></a>. 
                         </li>
			<li> (Mar 2023) I will be serving as an Area Chair for NeurIPS 2023.
                         </li>
			<li> (Mar 2023) Glad to give an invited talk at SIAM Southeastern Atlantic Section Annual Meeting about new federated and adaptive optimization algorithms for deep learning with unbounded smooth landscape. 
			</li>
			<li> (Mar 2023) Glad to give an invited talk at IBM Almaden Research Center about new federated and adaptive optimization algorithms for deep learning with unbounded smooth landscape. 
                         </li>
			<li> (Feb 2023) Glad to give an invited talk at Google about new federated and adaptive optimization algorithms for deep learning with unbounded smooth landscape. 
                         </li>
			
			<li> (Jan 2023) One paper was accepted by ICLR 2023. Congratulations to my students!
                         </li>
			<li> (Nov 2022) Two NeurIPS 2022 papers <a href="https://arxiv.org/pdf/2205.05040.pdf"><font color="#0000FF">Communication-Efficient Distributed Gradient Clipping</font></a> and <a href="https://arxiv.org/pdf/2205.14224.pdf"><font color="#0000FF">Bilevel Optimization</font></a> were selected as spotlight presentations (5% acceptance rate).
                         </li>
			<li> (Sep 2022) Three papers were accepted by NeurIPS 2022. Congratulations to my students and co-authors.
                         </li>
                         <li> (May 2022) One paper was accepted by ICML 2022. Congratulations to my students and co-authors.
			 </li>
			<li> (Mar 2022) Gave a talk at CISS at Princeton University about nonconvex-nonconcave min-max optimization and applications in GANs.
                            </li>
                         <li> (Feb 2022) We will give a tutorial on CVPR 2022 about "Deep AUC Maximization". Here is the <a href="https://libauc.org/cvpr2022/" target="_blank"><font color="#0000FF">website</font></a>.
                         </li>
-->

<!--                          <li> (Feb 2022) Two papers were accepted by ALT 2022.
                            </li>
                        <li> (Sep 2021) One paper was accepted by NeurIPS 2021.
                            </li>
                         <li> (Aug 2021) Joined in CS @ GMU as an assistant professor.
                         </li> 
                        <li> (June 2021) The paper about nonconvex-nonconcave min-max optimization has been accepted by JMLR.
                         <br>
                         </li>
                        <li> (Sep 2020) Two papers were accepted by NeurIPS 2020, with one Spotlight presentation (3% acceptance rate).
                         <br>
                         </li>
                        <li> (May 2020) One paper was accepted by ICML 2020.
                         <br>
                         </li>

                         <li> (Dec 2019) Two papers were accepted by ICLR 2020.
                         <br>
                         </li>

                         <li> (Sep 2018) Three papers were accepted by NeurIPS 2018.
                         <br>
                         </li>
                         <li> (May 2018) One paper was accepted by ICML 2018.
                         <br>
                        </li> -->

                 <ul>
                         </div>
                </div>

 



         <div class="education">
                <h3 class="clr1">Recent Selected Publications <font color="#5733FF"><a href="publications.html"><b>[Full List]</b></a></font></h3>
                <div class="skill_info">
                <ul>
                    #: supervised student author, *: equal contribution (alphabetical order)
                    <br><br>
	       <!--<li>(<FONT COLOR="#FC0404">New! </font>)
                             <a href="https://openreview.net/pdf?id=X7-y2_vjvk"><font color="#0000FF">AUC Maximization in Imbalanced Lifelong Learning</font></a><br>
                         Xiangyu Zhu<sup>#</sup>, Jie Hao<sup>#</sup>, Yunhui Guo, <b>Mingrui Liu</b>.<br>
          To appear in the 39th <em>Conference on Uncertainty in Artificial Intelligence</em>, 2023. (<b>UAI</b> 2023) 
       <br><br>
     </li>-->

<li>
                (<FONT COLOR="#FC0404">New! </font>)  <a href="https://arxiv.org/pdf/2506.13974"><font color="#0000FF">Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability</font></a><br>
                        Michael Crawshaw<sup>#</sup>, Blake Woodworth, <b>Mingrui Liu</b>.<br>
          Proceedings of 42th <em>International Conference on Machine Learning</em>, 2025. (<b>ICML</b> 2025) 
       <br><br>
     </li>
     <li>
                (<FONT COLOR="#FC0404">New! </font>)  <a href="https://arxiv.org/pdf/2501.13790"><font color="#0000FF">Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression</font></a><br>
                        Michael Crawshaw<sup>#</sup>, Blake Woodworth, <b>Mingrui Liu</b>.<br>
          In 13th <em>International Conference on Learning Representations</em>, 2025. (<b>ICLR</b> 2025) <FONT COLOR="#FC0404">  </font> 
       <br><br>
     </li>
        <li>
                (<FONT COLOR="#FC0404">New! </font>)  <a href="https://openreview.net/pdf?id=ZjOXuAfS6l"><font color="#0000FF">Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness</font></a><br>
                        Michael Crawshaw<sup>#</sup>, <b>Mingrui Liu</b>.<br>
          In 13th <em>International Conference on Learning Representations</em>, 2025. (<b>ICLR</b> 2025) <FONT COLOR="#FC0404">  </font> 
       <br><br>
     </li>

 
          <li>
                  <a href="https://arxiv.org/pdf/2410.23131"><font color="#0000FF">Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis</font></a><br>
                        Michael Crawshaw<sup>#</sup>, <b>Mingrui Liu</b>.<br>
          In Advances in <em>Neural Information Processing Systems 37</em>, 2024. (<b>NeurIPS</b> 2024)  </font> 
       <br><br>
     </li>


               <li>
                 <a href="https://arxiv.org/pdf/2409.19212"><font color="#0000FF">An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness</font></a><br>
                        Xiaochuan Gong<sup>#</sup>, Jie Hao<sup>#</sup>, <b>Mingrui Liu</b>.<br>
          In Advances in <em>Neural Information Processing Systems 37</em>, 2024. (<b>NeurIPS</b> 2024)  </font> 
       <br><br>
     </li>


          <li>
                         <a href="https://openreview.net/pdf?id=yHRxnhKyEJ"><font color="#0000FF">Provable Benefits of Local Steps in Heterogeneous Federated Learning for Neural Networks: A Feature Learning Perspective</font></a><br>
                         Yajie Bao<sup>#</sup>, Michael Crawshaw<sup>#</sup>, <b>Mingrui Liu</b>. <br>
                         Proceedings of 41th <em>International Conference on Machine Learning</em>, 2024. (<b>ICML</b> 2024) <br>
       <br>
     </li>

         <li>
                         <a href="https://openreview.net/pdf?id=36rWa8zVkh"><font color="#0000FF">A Nearly Optimal Single Loop Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness</font></a><br>
                         Xiaochuan Gong<sup>#</sup>, Jie Hao<sup>#</sup>, <b>Mingrui Liu</b>. <br>
                         Proceedings of 41th <em>International Conference on Machine Learning</em>, 2024. (<b>ICML</b> 2024) <br>
       <br>
     </li>


       <li>
                  <a href="https://arxiv.org/pdf/2401.09587.pdf"><font color="#0000FF">Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis </font></a><br>
                        Jie Hao<sup>#</sup>, Xiaochuan Gong<sup>#</sup>, <b>Mingrui Liu</b>.<br>
          In 12th <em>International Conference on Learning Representations</em>, 2024. (<b>ICLR</b> 2024) <FONT COLOR="#FC0404"> (Spotlight, 5% acceptance rate) </font>
       <br><br>
     </li>
          <li>
                  <a href="https://openreview.net/pdf?id=Yq6GKgN3RC"><font color="#0000FF">Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds</font></a><br>
                        Michael Crawshaw<sup>#</sup>, Yajie Bao<sup>#</sup>, <b>Mingrui Liu</b>.<br>
          In Advances in <em>Neural Information Processing Systems 36</em>, 2023. (<b>NeurIPS</b> 2023)  </font> 
       <br><br>
     </li>
          <li>
                  <a href="https://openreview.net/pdf?id=gVLKXT9JwG"><font color="#0000FF">Global Convergence Analysis of Local SGD for Two-layer Neural Network without Overparameterization</font></a><br>
                        Yajie Bao<sup>#</sup>, Amarda Shehu, <b>Mingrui Liu</b>.<br>
          In Advances in <em>Neural Information Processing Systems 36</em>, 2023. (<b>NeurIPS</b> 2023)  </font> 
       <br><br>
     </li>
            <li>
                  <a href="https://openreview.net/pdf?id=2dtU9ZbgSN"><font color="#0000FF">Bilevel Coreset Selection in Continual Learning: A New Formulation and Algorithm</font></a><br>
                        Jie Hao<sup>#</sup>, Kaiyi Ji, <b>Mingrui Liu</b>.<br>
          In Advances in <em>Neural Information Processing Systems 36</em>, 2023. (<b>NeurIPS</b> 2023)  </font> 
       <br><br>
     </li>
               <li>
                             <a href="https://openreview.net/pdf?id=ytZIYmztET"><font color="#0000FF">EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data</font></a><br>
                         Michael Crawshaw<sup>#</sup>, Yajie Bao<sup>#</sup>, <b>Mingrui Liu</b>.<br>
          In 11th <em>International Conference on Learning Representations</em>, 2023. (<b>ICLR</b> 2023) 
       <br><br>
     </li>
                    <li>
                             <a href="https://arxiv.org/pdf/2205.05040.pdf"><font color="#0000FF">A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks</font></a><br>
                         <b>Mingrui Liu</b>, Zhenxun Zhuang, Yunwen Lei, Chunyang Liao.<br>
          In Advances in <em>Neural Information Processing Systems 35</em>, 2022. (<b>NeurIPS</b> 2022)<FONT COLOR="#FC0404"> (Spotlight, 5% acceptance rate) </font> 
       <br><br>
     </li>
                        <li>
                             <a href="https://arxiv.org/pdf/2208.11195.pdf"><font color="#0000FF">Robustness to Unbounded Smoothness of Generalized SignSGD</font></a><br>
                         Michael Crawshaw<sup>#*</sup>, <b>Mingrui Liu<sup>*</sup></b>, Francesco Orabona<sup>*</sup>, Wei Zhang<sup>*</sup>, Zhenxun Zhuang<sup>*</sup>.<br>
          In Advances in <em>Neural Information Processing Systems 35</em>, 2022. (<b>NeurIPS</b> 2022)
       <br><br>
     </li>
               
                        <li>
                             <a href="https://arxiv.org/pdf/2205.14224.pdf"><font color="#0000FF">Will Bilevel Optimizers Benefit from Loops</font></a><br>
                         Kaiyi Ji, <b>Mingrui Liu</b>, Yingbin Liang, Lei Ying.<br>
          In Advances in <em>Neural Information Processing Systems 35</em>, 2022. (<b>NeurIPS</b> 2022) <FONT COLOR="#FC0404"> (Spotlight, 5% acceptance rate) </font> 
       <br><br>
     </li>
               

                 <li>
                         <a href="https://arxiv.org/pdf/2207.08204.pdf"><font color="#0000FF">Fast Composite Optimization and Statistical Recovery in Federated Learning</font></a><br>
                         Yajie Bao<sup>#</sup>, Michael Crawshaw<sup>#</sup>, Shan Luo, <b>Mingrui Liu</b>. <br>
                         Proceedings of 39th <em>International Conference on Machine Learning</em>, 2022. (<b>ICML</b> 2022) <br>
       <br>
     </li>

       <li>
                             <a href="https://openreview.net/pdf?id=IKhEPWGdwK"><font color="#0000FF">Understanding AdamW through Proximal Methods and Scale-Freeness</font></a><br>
                         Zhenxun Zhuang, <b>Mingrui Liu</b>, Ashok Cutkosky, Francesco Orabona.<br>
       <em>Transactions on Machine Learning Research</em>, 2022. (<b>TMLR</b> 2022)
       <br><br>
     </li>
                        <li>
                             <a href="https://arxiv.org/pdf/2103.00284.pdf"><font color="#0000FF">On the Initialization for Convex-Concave Min-max Problems</font></a><br>
                         <b>Mingrui Liu</b>, Francesco Orabona.<br>
       <em>Algorithmic Learning Theory</em>, 2022. (<b>ALT</b> 2022)
       <br><br>
     </li>
                              <li>
                             <a href="https://arxiv.org/pdf/2102.07002.pdf"><font color="#0000FF">On the Last Iterate Convergence of Momentum Methods</font></a><br>
                         Xiaoyu Li, <b>Mingrui Liu</b>, Francesco Orabona. <br>
        <em>Algorithmic Learning Theory</em>, 2022. (<b>ALT</b> 2022)
              <br><br>
     </li>
                     <li>
                         <a href="https://openreview.net/pdf?id=lHFCaetMa_"><font color="#0000FF">Generalization Guarantee of SGD for Pairwise Learning</font></a><br>
                         Yunwen Lei, <b>Mingrui Liu</b>, Yiming Ying.<br>
       Advances in <em>Neural Information Processing Systems 34</em>, 2021. (<b>NeurIPS</b> 2021)
       <br><br>
     </li>
			 <!-- <li>
                             <a href="https://arxiv.org/pdf/2205.05040.pdf"><font color="#0000FF">A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks</font></a><br>
                         <b>Mingrui Liu</b>, Zhenxun Zhuang, Yunwen Lei, Chunyang Liao.<br>
		To appear in Advances in <em>Neural Information Processing Systems 36</em>, 2022. (<b>NeurIPS</b> 2022) <FONT COLOR="#FC0404"> (Spotlight, 5% acceptance rate) </font> 
       <br><br>
     </li> -->
                      <!--   <li>
                             <a href="https://arxiv.org/abs/1810.10207.pdf"><font color="#0000FF">First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems</font></a><br>
                         <b>Mingrui Liu</b>, Hassan Rafique, Qihang Lin, Tianbao Yang.<br>
                         <em>Journal of Machine Learning Research</em>, 2021. (<b>JMLR</b> 2021)
       <br><br>
     </li>
            <li>
                         <a href="https://arxiv.org/pdf/1810.02060.pdf"><font color="#0000FF">Non-Convex Min-Max Optimization: Provable Algorithms and Applications in Machine Learning</font></a><br>
                         Hassan Rafique, <b>Mingrui Liu</b>, Qihang Lin, Tianbao Yang.<br>
       <em>Optimization Methods and Software</em>, 2021.
       <br><br>
     </li>
          <li>
                         <a href="https://arxiv.org/abs/1909.11763"><font color="#0000FF">Improved Schemes for Episodic Memory-based Lifelong Learning</font></a><br>
                         Yunhui Guo*, <b>Mingrui Liu</b>*, Tianbao Yang, Tajana Rosing. (*: equal contribution) <br>
                         Advances in <em>Neural Information Processing Systems 33</em>, 2020. (<b>NeurIPS</b> 2020)  <FONT COLOR="#FC0404"> (Spotlight, 3% acceptance rate, top 4% submissions) </font> <br>
        [<a href="https://github.com/yunhuiguo/MEGA" target="_blank"><font color="0000FF">Code</font></a>]
       <br><br>
     </li>

               <li>
                         <a href="https://arxiv.org/abs/1910.12999.pdf"><font color="#0000FF">A Decentralized Parallel Algorithm for Training Generative Adversarial Nets</font></a><br>
                         <b>Mingrui Liu</b>, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jerret Ross, Tianbao Yang, Payel Das.<br>
       Advances in <em>Neural Information Processing Systems 33</em>, 2020. (<b>NeurIPS</b> 2020)
       <br><br>
     </li>

 <li>
                     <a href="https://openreview.net/forum?id=SJxIm0VtwH"><font color="#0000FF">Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets</font></a><br>
                         <b>Mingrui Liu</b>, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, Tianbao Yang.<br>
                         
                          8th <em>International Conference on Learning Representations</em>, 2020. (<b>ICLR</b> 2020)
                    <br>
                                      
                    <br>
                    </li>

     <li>                
                         <a href="https://openreview.net/forum?id=HJepXaVYDr"><font color="#0000FF">Stochastic AUC Maximization with Deep Neural Networks</font></a><br>
                         <b>Mingrui Liu</b>, Zhuoning Yuan, Yiming Ying, Tianbao Yang.<br>
                         8th <em>International Conference on Learning Representations</em>, 2020. (<b>ICLR</b> 2020) <br>
                         [<a href="https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y" target="_blank"><font color="0000FF">Code</font></a>]

       <br><br>
     </li>

           

                   <li>
                    <a href="papers/fastAUC_ICML2018.pdf"><font color="#0000FF">Fast Stochastic AUC Maximization with O(1/n) Convergence Rate</font></a><br>
                         <b>Mingrui Liu</b>, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, Tianbao Yang.<br>
                         Proceedings of the 35th <em>International Conference on Machine Learning 35</em>, 2018. (<b>ICML</b> 2018)
                         <br>
                    [<a href="papers/fastAUC_supp.pdf" target="_blank"><font color="#0000FF">Supplement</font></a>] [<a href="papers/fastAUC_ICML18_bib.html" target="_blank"><font color="#0000FF">Bibtex</font></a>]  [<a href="papers/ICML18_AUC_poster.pdf" target="_blank"><font color="0000FF">Poster</font></a>] [<a href="codes/src_FSAUC.zip" target="_blank"><font color="#0000FF">Code</font></a>] <br>
               <br>
                    </li> -->

               </ul>
              </div>
          </div>

         
     <div class="copywrite">
			 <p>Last update: 08-01-2025</p>
		 </div>


	 </div>

</div>

</body>
</html>
